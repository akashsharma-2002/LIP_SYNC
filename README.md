# LIP_SYNC


A deep fake lip sync model leveraging the pre-trained capabilities of Wav2Lip represents a groundbreaking fusion of audio and video manipulation. Wav2Lip, originally designed for synchronizing lip movements with audio, serves as the foundation. This pre-trained model uses a combination of lip landmark detection and deep learning techniques to generate realistic lip movements that align perfectly with a given audio input.

By fine-tuning this model on specific datasets, users can create convincing deep fake videos where the lip movements of a target person are seamlessly synchronized with an alternate audio source. The result is hyper-realistic audio-visual manipulation with potentially concerning implications for misinformation and privacy.

This model is tested with with video_1.mp4 and auido_1.wav and tried using lip sync which has given a wonderful result stored in result_voice.mp4 and result_voice_1.mp4
